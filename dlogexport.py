#!/usr/bin/env python

import argparse
import csv
import decimal
import itertools
import json
import sys

from dlog import Dlog

# https://stackoverflow.com/a/46841935/7011381
class SerializableGenerator(list):
    """Generator that is serializable by JSON

    It is useful for serializing huge data by JSON
    >>> json.dumps(SerializableGenerator(iter([1, 2])))
    "[1, 2]"
    >>> json.dumps(SerializableGenerator(iter([])))
    "[]"

    It can be used in a generator of json chunks used e.g. for a stream
    >>> iter_json = ison.JSONEncoder().iterencode(SerializableGenerator(iter([])))
    >>> tuple(iter_json)
    ('[1', ']')
    # >>> for chunk in iter_json:
    # ...     stream.write(chunk)
    # >>> SerializableGenerator((x for x in range(3)))
    # [<generator object <genexpr> at 0x7f858b5180f8>]
    """

    def __init__(self, iterable):
        tmp_body = iter(iterable)
        try:
            self._head = iter([next(tmp_body)])
            self.append(tmp_body)
        except StopIteration:
            self._head = []

    def __iter__(self):
        return itertools.chain(self._head, *self[:1])


class _Dlog(Dlog):
    """Kaitai <0.9 workaroumd: Munge the openlogger channel_map property.

    Kaitai v0.9 is required to implement the conditional (openscope vs
    openlogger) definition for the channel_map instance. In the meantime we can
    DIY: this is the same code as generated by the beta 0.9 compiler when the
    channel_map is enabled. Once Kaitai 0.9 is released this can be ditched.
    """
    class Body(Dlog.Body):
        class Header(Dlog.Body.Header):
            @property
            def channel_map(self):
                if hasattr(self, '_m_channel_map'):
                    return self._m_channel_map if hasattr(self, '_m_channel_map') else None  # noqa: E501

                self._m_channel_map = (self.openlogger_channel_map if self.dlog_format == self._root.DlogFormats.openlogger else [0])  # noqa: E501
                return self._m_channel_map if hasattr(self, '_m_channel_map') else None  # noqa: E501


def extract_header_info(header: Dlog.Body.Header) -> list:
    """Extract the information from the header in a presentable form.

    Args:
        header: a Dlog header object.
    Returns:
        list: header info as formatted strings
    """

    voltage_scale_strings = {1000: "mV", 1: "V"}
    voltage_scale = 1/header.voltage_scale
    if header.voltage_scale in voltage_scale_strings:
        voltage_scale = voltage_scale_strings[header.voltage_scale]

    sample_rate = decimal.Decimal(header.sample_rate)
    delay = decimal.Decimal(header.delay)

    info = [f'log format: {header.dlog_format.name}',
            f'stop reason: {header.stop_reason.name}',
            f'number of samples: {header.num_samples}',
            f'voltage units: {voltage_scale}',
            f'sample rate: {sample_rate.normalize().to_eng_string()} Sa/s',
            f'delay: {delay.normalize().to_eng_string()} s',
            f'number of channels: {header.num_channels}',
            f'channel map: {header.channel_map[:header.num_channels]}']

    return info


def write_csv(data, meta_data=None, column_header=None, file=sys.stdout):
    csv_writer = csv.writer(file)
    if meta_data is not None:
        csv_writer.writerow([json.dumps(meta_data)])
    if column_header is not None:
        csv_writer.writerow(column_header)
    csv_writer.writerows(data)


def write_json(data, meta_data=None, column_header=None, file=sys.stdout):
    json.dump(SerializableGenerator(iter(data)), file)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('inputfile', help='input log file')
    parser.add_argument('-f', '--format', choices=['csv', 'json'],
                        default='csv', help='output format')
    parser.add_argument('--no-metadata', action='store_true',
                        help='omit log meta-data')
    parser.add_argument('--no-csv-header', action='store_true',
                        help='omit header for CSV output')
    args = parser.parse_args()

    dlog = _Dlog.from_file(args.inputfile)

    header = dlog.body.header
    header_info = extract_header_info(header)
    print('\n'.join(['Header Information'] + header_info), file=sys.stderr)

    timestamped_data = ([i/header.sample_rate, *s.channel]
                        for i, s in enumerate(dlog.body.data.samples))

    meta_data = column_header = None
    if not args.no_metadata:
        meta_data = header_info
    if not args.no_csv_header:
        column_header = ['Time'] + ['Channel ' + str(c) for c in
                                    header.channel_map[:header.num_channels]]

    if args.format == 'json':
        write_json(timestamped_data, meta_data, column_header)
    else:
        write_csv(timestamped_data, meta_data, column_header)


if __name__ == "__main__":
    main()
